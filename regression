# Polynomial Regression:

data:
Var_X,Var_Y
-0.33532,6.66854
0.02160,3.86398
-1.19438,5.16161
-0.65046,8.43823
-0.28001,5.57201
1.93258,-11.13270
1.22620,-5.31226
0.74727,-4.63725
3.32853,3.80650
2.87457,-6.06084
-1.48662,7.22328
0.37629,2.38887
1.43918,-7.13415
0.24183,2.00412
-2.79140,4.29794
1.08176,-5.86553
2.81555,-5.20711
0.54924,-3.52863
2.36449,-10.16202
-1.01925,5.31123


import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# Assign the data to predictor and outcome variables
# TODO: Load the data
train_data = pd.read_csv('data.csv')
X = train_data['Var_X'].values.reshape(-1, 1)
y = train_data['Var_Y'].values

# Create polynomial features
# TODO: Create a PolynomialFeatures object, then fit and transform the predictor feature
poly_feat = PolynomialFeatures(degree = 4)  # x^0,x^1,x^2,x^3,x^4
X_poly = poly_feat.fit_transform(X)  #  the "fit" side of the method considers how many features are needed in the output, 
                                        and the "transform" side applies those considerations to the data provided to the method 
                                        as an argument.
                                        Assign the new feature matrix to the X_poly variable.
                                        
# Make and fit the polynomial regression model
# TODO: Create a LinearRegression object and fit it to the polynomial predictor features
poly_model = LinearRegression(fit_intercept = False).fit(X_poly, y)

